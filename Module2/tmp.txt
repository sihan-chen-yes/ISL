fitter

```Python
def attack_batch(self, x, y):
        """
        Attack a batch of images with targeted PGD while also evading detection.
        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)
        :param y: the labels of size (batch_size,)
        :return: the adversarial images of size (batch_size, 3, 224, 224)
        """
        x_adv = x.clone()
        for i in range(self.steps):
          self.fit(x_adv)

          x_adv = x_adv.requires_grad_(True)

          loss_func = torch.nn.CrossEntropyLoss()

          logits_adv = self.clf.get_logits(x_adv)

          binary_preds = (self.clf.get_logits(x_adv) @ self.fitter).to(device)

          binary_target = torch.zeros(binary_preds.shape[0], dtype=torch.long).to(device)

          loss = -loss_func(logits_adv, y) + loss_func(binary_preds, binary_target)

          loss.backward()

          grad = x_adv.grad.detach()

          with torch.no_grad():
            x_adv = x_adv - self.alpha * torch.sign(grad)
            x_adv = self.project(x_adv, x)

        return x_adv

    def attack_all(self, images, labels, batch_size=20):
        self.fit(images)
        return utils.batched_func(self.attack_batch, inputs=(images, labels),
                                  batch_size=batch_size,
                                  device=self.clf.device)

    def fit(self, images):
      num = images.shape[0]
      batch_size = 50
      iter = num // batch_size
      for j in range(max(1, iter)):
        if(iter == 1):
          batch_images = images.to(device)
        else:
          batch_images = images[j * batch_size:(j+1) * batch_size].to(device)
        for i in range(self.epochs):
          self.fitter = self.fitter.requires_grad_(True)

          logits = self.clf.get_logits(batch_images) @ self.fitter

          labels = self.clf.detect(batch_images).to(device).type(torch.long)

          loss_func = torch.nn.CrossEntropyLoss()
          loss = loss_func(logits, labels)
          loss.backward()

          grad = self.fitter.grad.detach()

          with torch.no_grad():
            self.fitter = self.fitter - self.lr * grad
```

pretend the logits the same

```Python
    def attack_batch(self, x, y):
        """
        Attack a batch of images with targeted PGD while also evading detection.
        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)
        :param y: the labels of size (batch_size,)
        :return: the adversarial images of size (batch_size, 3, 224, 224)
        """
        x_adv = x.clone()
        for i in range(self.steps):
          x_adv = x_adv.requires_grad_(True)

          loss_func = torch.nn.CrossEntropyLoss()
          loss_func2 = torch.nn.MSELoss()

          logits_adv = self.clf.get_logits(x_adv)
          logits_orig = self.clf.get_logits(x)

          loss = -loss_func(logits_adv, y) + loss_func2(logits_adv, logits_orig)

          loss.backward()

          grad = x_adv.grad.detach()

          with torch.no_grad():
            x_adv = x_adv - self.alpha * torch.sign(grad)
            x_adv = self.project(x_adv, x)

        return x_adv
```

target pretend:√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√ 我日

```Python
    def attack_batch(self, x, y):
        """
        Attack a batch of images with targeted PGD while also evading detection.
        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)
        :param y: the labels of size (batch_size,)
        :return: the adversarial images of size (batch_size, 3, 224, 224)
        """
        x_adv = x.clone()
        logits = self.clf.get_logits(x)
        _, indices = torch.topk(logits, 2)
        y_targets = indices[:, 1]
        for i in range(self.steps):
          x_adv = x_adv.requires_grad_(True)

          loss_func = torch.nn.CrossEntropyLoss()
          loss_func2 = torch.nn.L1Loss()

          logits_adv = self.clf.get_logits(x_adv)
          logits_orig = self.clf.get_logits(x)

          loss = loss_func(logits_adv, y_targets) + loss_func2(logits_adv, logits_orig)

          loss.backward()

          grad = x_adv.grad.detach()

          with torch.no_grad():
            x_adv = x_adv - self.alpha * torch.sign(grad)
            x_adv = self.project(x_adv, x)

        return x_adv
```





每次decode回去：

```Python
    def attack_batch(self, x, y_targets):
        """
        Attack a batch of images with targeted PGD while also evading random preprocessing.
        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)
        :param y_targets: the target labels of size (batch_size,)
        :return: the adversarial images of size (batch_size, 3, 224, 224)
        """
        x_adv = x.clone()
        x_adv_enc = self.clf.encode(x_adv / 255.0)
        for i in range(self.steps):
          x_adv_enc = x_adv_enc.requires_grad_(True)

          loss_func = torch.nn.CrossEntropyLoss()

          logits = self.clf.model(x_adv_enc)

          loss = loss_func(logits, y_targets)

          loss.backward()

          grad = x_adv_enc.grad.detach()

          with torch.no_grad():
            x_adv_enc = x_adv_enc - self.alpha * torch.sign(grad)

        x_adv_enc = torch.round(x_adv_enc)
        x_adv = self.decode(x_adv_enc)
        x_adv = self.project(x_adv, x)

        return x_adv
      
    def decode(self, x_enc):
      #from x_enc of B, 60, 224, 224 to x_orig of B, 3, 224, 224
      x_enc = x_enc.permute(0, 2, 3, 1)
      x_1 = x_enc[:, :, :, 0:20].sum(dim=3, keepdim=True) / 20
      x_2 = x_enc[:, :, :, 20:40].sum(dim=3, keepdim=True) / 20
      x_3 = x_enc[:, :, :, 40:].sum(dim=3, keepdim=True) / 20
      x_orig = torch.cat((x_1, x_2, x_3), 3)
      x_orig = x_orig.permute(0, 3, 1, 2)
      x_orig = x_orig.to(x_enc.device)
      return x_orig
```





```Python
def change_logits_form(orig_logits):
  logits = np.zeros_like(orig_logits)
  for i in range(num_classes):
    mask_y = np.zeros_like(orig_logits)
    mask_y[:, :, i] = 1
    mask_y_prime = abs(mask_y - 1)
    y_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y)
    y_prime_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y_prime)
    logits[:, :, i] = y_logits - y_prime_logits
  
  return logits

def disentangle(logits):
  #for a sample
  indices = np.argsort(logits)
  num = num_models // 2
  return indices[:num + 1], indices[num + 1:]

def get_probs(x, data):
  mu, std = scipy.stats.norm.fit(data)
  probs = scipy.stats.norm.pdf(x, mu, std)
  return probs

all_orig_logits = np.concatenate((activations_fit, np.expand_dims(activations_attack, axis=1)), axis=1)    
logits = change_logits_form(all_orig_logits)

label_indices = np.expand_dims(np.expand_dims(labels, axis=1).repeat(128, axis=1), axis=2)
label_logits = np.take_along_axis(logits, label_indices, axis=2)
label_logits = label_logits.reshape(num_samples, -1)

target_logits = label_logits[:, -1]
shadow_logits = label_logits[:, :-1]

ratios = []
for i in range(num_samples):
  out_indices, in_indices = disentangle(shadow_logits[i])
  out_logits = shadow_logits[i, out_indices]
  in_logits = shadow_logits[i, in_indices]
  ratio = get_probs(target_logits[i], in_logits) / get_probs(target_logits[i], out_logits)
  ratios.append(ratio)

# TODO: Replace the following dummy score with your attack
attack_scores = np.array(ratios, dtype=np.float32)

# Save scores
utils.save_attack_scores(os.path.join(RESULTS_PATH, "attack_scores.npy"), attack_scores)
```

暴力切割cluster :

```Python
indices = np.argsort(logits)
num = num_models // 2
return indices[:num + 1], indices[num + 1:]
```

```Python
```



2.GMM分离效果不好

```Python
  gm = sklearn.mixture.GaussianMixture(n_components=2, random_state=0, tol=1e-5, max_iter=500)
  logits = np.expand_dims(logits, axis=1)
  cluster = gm.fit_predict(logits)
  out_indices = cluster == 0
  in_indices = cluster == 1
  return out_indices, in_indices
```

Kmeans:

```Python
    indices = np.argsort(logits)
  mean0_index = num_models // 4
  mean1_index = num_models * 3 // 4
  means = np.array([logits[indices[mean0_index]], logits[indices[mean1_index]]])
  means = means.reshape(2, -1)
   logits = np.expand_dims(logits, axis=1)
  kmeans = KMeans(n_clusters=2, random_state=0, n_init="auto", init=means).fit(logits)
  cluster = kmeans.labels_
  out_indices = cluster == 0
  in_indices = cluster == 1
  return out_indices, in_indices
```

```Python
def change_logits_form(orig_logits):
  logits = np.zeros_like(orig_logits)
  for i in range(num_classes):
    #mask:num_samples, num_models, num_classes
    mask_y = np.zeros_like(orig_logits)
    mask_y[:, :, i] = 1
    mask_y_prime = abs(mask_y - 1)
    y_logits = (orig_logits * mask_y).max(axis=2)
    y_prime_logits = (orig_logits * mask_y_prime).max(axis=2)
    # y_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y)
    # y_prime_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y_prime)
    logits[:, :, i] = y_logits - y_prime_logits

  return logits

def disentangle(logits):
  #TODO
  # for a sample
  out_logits, in_logits = disentangle_with_Gaussian(logits)
  #if can't disentangle with gaussian

  return out_logits, in_logits

def disentangle_with_Gaussian(logits):
  sorted_logits = np.sort(logits)
  mean0_index = num_models // 4
  mean1_index = num_models * 3 // 4
  num = num_models // 2
  means = np.array([sorted_logits[mean0_index], sorted_logits[mean1_index]])
  means = means.reshape(2, -1)

  gm = GaussianMixture(n_components=2, random_state=0, n_init=2, means_init=means)

  logits = np.expand_dims(logits, axis=1)

  cluster = gm.fit_predict(logits)

  out_indices = cluster == 0
  in_indices = cluster == 1

  out_logits = logits[out_indices].squeeze(axis=1)
  in_logits = logits[in_indices].squeeze(axis=1)

  return out_logits, in_logits

def get_prob(x, data):
  mu, std = scipy.stats.norm.fit(data)
  prob = scipy.stats.norm.pdf(x, mu, std)
  return prob

all_orig_logits = np.concatenate((activations_fit, np.expand_dims(activations_attack, axis=1)), axis=1)
logits = change_logits_form(all_orig_logits)

label_indices = np.expand_dims(np.expand_dims(labels, axis=1).repeat(num_models + 1, axis=1), axis=2)
label_logits = np.take_along_axis(logits, label_indices, axis=2)
label_logits = label_logits.reshape(num_samples, -1)


target_logits = label_logits[:, -1]
shadow_logits = label_logits[:, :-1]

ratios = []
for i in range(num_samples):
  out_logits, in_logits = disentangle(shadow_logits[i])
  # print(out_logits)
  # print(in_logits)
  # print(len(out_logits))
  # print(len(in_logits))

  ratio = get_prob(target_logits[i], in_logits) / get_prob(target_logits[i], out_logits)

  ratios.append(ratio)

# TODO: Replace the following dummy score with your attack
attack_scores = np.array(ratios, dtype=np.float32)

# Save scores
utils.save_attack_scores(os.path.join(RESULTS_PATH, "attack_scores.npy"), attack_scores)
```



```Python
def change_logits_form(orig_logits):
  logits = np.zeros_like(orig_logits)
  for i in range(num_classes):
    #mask:num_samples, num_models, num_classes
    mask_y = np.zeros_like(orig_logits)
    mask_y[:, :, i] = 1
    mask_y_prime = abs(mask_y - 1)
    y_logits = (orig_logits * mask_y).max(axis=2)
    y_prime_logits = (orig_logits * mask_y_prime).max(axis=2)
    # y_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y)
    # y_prime_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y_prime)
    logits[:, :, i] = y_logits - y_prime_logits

  return logits

def disentangle(logits):
  #TODO
  mark = np.zeros((num_samples, num_models))
  # for a sample
  for i in range(num_models):
    #logits per model
    logits_per_model = logits[:,i]

    mark_per_model = disentangle_with_Gaussian(logits_per_model)
    mark[:, i] = mark_per_model

  return mark

def disentangle_with_Gaussian(logits):
  sorted_logits = np.sort(logits)

  # mean0_index = num_models // 4
  # mean1_index = num_models * 3 // 4

  # means = np.array([sorted_logits[mean0_index], sorted_logits[mean1_index]])
  # means = means.reshape(2, -1)

  gmm = GaussianMixture(n_components=2, random_state=0)

  logits = np.expand_dims(logits, axis=1)

  cluster = gmm.fit_predict(logits)

  means = gmm.means_

  outn = len(cluster[cluster == 0])
  inn = len(cluster[cluster == 1])
  print(outn)
  print(inn)


  return cluster

def get_prob(x, data):
  mu, std = scipy.stats.norm.fit(data)
  prob = scipy.stats.norm.pdf(x, mu, std)
  return prob

all_orig_logits = np.concatenate((activations_fit, np.expand_dims(activations_attack, axis=1)), axis=1)
logits = change_logits_form(all_orig_logits)

label_indices = np.expand_dims(np.expand_dims(labels, axis=1).repeat(num_models + 1, axis=1), axis=2)
label_logits = np.take_along_axis(logits, label_indices, axis=2)
label_logits = label_logits.reshape(num_samples, -1)


target_logits = label_logits[:, -1]
shadow_logits = label_logits[:, :-1]

mark = disentangle(shadow_logits)
print(mark[0])
ratios = []
for i in range(num_samples):
  out_logits = shadow_logits[i, mark[i] == 0]
  in_logits = shadow_logits[i, mark[i] == 1]
  # print(out_logits)
  # print(in_logits)
  # print(len(out_logits))
  # print(len(in_logits))

  ratio = get_prob(target_logits[i], in_logits) / get_prob(target_logits[i], out_logits)

  ratios.append(ratio)

# TODO: Replace the following dummy score with your attack
attack_scores = np.array(ratios, dtype=np.float32)

# Save scores
utils.save_attack_scores(os.path.join(RESULTS_PATH, "attack_scores.npy"), attack_scores)


```

```Python
def change_logits_form(orig_logits):
  logits = np.zeros_like(orig_logits)
  for i in range(num_classes):
    #mask:num_samples, num_models, num_classes
    mask_y = np.zeros_like(orig_logits)
    mask_y[:, :, i] = 1
    mask_y_prime = abs(mask_y - 1)
    y_logits = (orig_logits * mask_y).max(axis=2)
    y_prime_logits = (orig_logits * mask_y_prime).max(axis=2)
    # y_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y)
    # y_prime_logits = scipy.special.logsumexp(a=orig_logits, axis=2, b=mask_y_prime)
    logits[:, :, i] = y_logits - y_prime_logits

  return logits

def disentangle(logits):
  #TODO
  # for a sample
  means, stds = disentangle_with_Gaussian(logits)

  return means, stds

def disentangle_with_Gaussian(logits):
  sorted_logits = np.sort(logits)
  mean0_index = num_models // 4
  mean1_index = num_models * 3 // 4

  means = np.array([sorted_logits[mean0_index], sorted_logits[mean1_index]])
  means = means.reshape(2, -1)

  gm = GaussianMixture(n_components=2, n_init=2, random_state=0, means_init=means, covariance_type='spherical')

  logits = np.expand_dims(sorted_logits, axis=1)

  gm.fit(logits)

  means = gm.means_.reshape(2)

  stds = gm.covariances_.reshape(2)

  if means[0] > means[1]:
    means = [means[1], means[0]]
    stds = [stds[1], stds[0]]

  assert means[0] < means[1]

  return means, stds

def get_prob(x, mean, std):
  prob = scipy.stats.norm.pdf(x, mean, std)
  return prob

all_orig_logits = np.concatenate((activations_fit, np.expand_dims(activations_attack, axis=1)), axis=1)
logits = change_logits_form(all_orig_logits)

label_indices = np.expand_dims(np.expand_dims(labels, axis=1).repeat(num_models + 1, axis=1), axis=2)
label_logits = np.take_along_axis(logits, label_indices, axis=2)
label_logits = label_logits.reshape(num_samples, -1)


target_logits = label_logits[:, -1]
shadow_logits = label_logits[:, :-1]

ratios = []
for i in range(num_samples):
  means, stds = disentangle(shadow_logits[i])
  # print(out_logits)
  # print(in_logits)
  # print(len(out_logits))
  # print(len(in_logits))
  in_likelihood = get_prob(target_logits[i], means[1], stds[1])
  if out_likelihood == 0:
    out_likelihood = get_prob(target_logits[i], means[0], stds[0]) + 1e-80
  ratio = in_likelihood / out_likelihood

  ratios.append(ratio)

# TODO: Replace the following dummy score with your attack
attack_scores = np.array(ratios, dtype=np.float32)

# Save scores
utils.save_attack_scores(os.path.join(RESULTS_PATH, "attack_scores.npy"), attack_scores)
```




